<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Md Jahidul Islam</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Md Jahidul Islam</strong> </a>
									<ul class="icons">
									    <li><a href="files/resume_mji.pdf" class="icon solid fa-file"><span class="label">Resume</span></a></li>
									    <li><a href="mailto: islam034@umn.edu" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
									    <li><a href="https://scholar.google.com/citations?user=XuEzu5cAAAAJ&hl=en" class="icon brands fa-google"><span class="label">Google-scholar</span></a></li>
									    <li><a href="https://www.researchgate.net/profile/Md_Jahidul_Islam6" class="icon brands fa-researchgate"><span class="label">Researchgate</span></a></li>
									    <li><a href="https://twitter.com/XahidBuffon" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="https://github.com/xahidbuffon" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
										<li><a href="https://www.linkedin.com/in/mdjahidulislam/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
									</ul>
								</header>

							<!-- Banner -->
								<section id="banner">
									<div class="content">
										<header>
											<h2>Hi, Iâ€™m Jahid</h2>
											 <!-- <p>Ph. D. Candidate, University of Minnesota.</p> -->
										</header>
										<p align="justify">
											I am a  Ph. D. candidate in computer science at the <a href="https://www.cs.umn.edu/">University of Minnesota (UMN), Twin Cities</a>. 
											I work at the <a href="http://irvlab.cs.umn.edu/">Interactive Robotics and Vision Lab (IRVLab)</a> and supervised by Prof. <a href="http://www-users.cs.umn.edu/~junaed/">Junaed Sattar</a>. My research interests lie in robot vision and deep learning, particularly in the design and development of robust deep visual modules for near real-time use by underwater robots. 
											I also explore research problems in the domains of applied machine learning and 3d computer vision. 
											Check out some of my <a href="#Proj">recent projects</a> and the <a href="#Pub">list of publications</a>; please feel free to <a href="mailto: islam034@umn.edu">reach out</a> and be <a href="https://www.linkedin.com/in/mdjahidulislam/">connected</a>. 
										</p>

										<br>
										<h4>About Me</h4> 
										<p align="justify">
											I am from <a href="https://en.wikipedia.org/wiki/Bangladesh"> Bangladesh</a>, a small and beautiful country in South Asia. I pursued my B.Sc. and M.Sc. degrees at <a href="http://www.buet.ac.bd/"> Bangladesh University of Eng. and Tech. (BUET)</a> on computer science and engineering (CSE) in the year 2012 and 2015, respectively.  
											I also worked as a Lecturer at the Dept. of CSE, <a href="http://www.uiu.ac.bd/">United Int. University (UIU)</a> during 2012-15. 
											Then I moved to <a href="https://en.wikipedia.org/wiki/Minneapolis">Minneapolis, the city of lakes</a> and started my Ph.D. marathon at the <a href="http://twin-cities.umn.edu/">UMN</a> in Fall 2015. <br> <br>

											Outside work, I play and follow cricket avidly. I also love traveling and taking pictures; checkout my <a href="#Extra">leisure pleasures</a> and lets talk if we have common interests! 
										</p>

									</div>
									<span class="image object">
										<img src="images/me.jpg" alt="" />
									</span>

								</section>

							<!-- Section -->
								<section, id="Enrol">
								<hr class="major" />
									<header class="major">
										<h2>Enrollments</h2>
									</header>
									<div class="posts">
										<article>
											<h3><i class="icon solid fas fa-microscope"></i> Graduate Research Assistant</h3>
											<h4><span style="font-weight:normal"> 
												<a href="https://www.cs.umn.edu/">Dept. of CSE, University of Minnesota</a> <br>
												Fall 2020/18, Summer 2020/17</a></span>
											</h4>
											<p align="justify"><i>
												<i class="fas fa-glasses"></i> My primary role as a research assistant involves working on several <a href="#Proj"> projects</a> at the   
												<a href="http://irvlab.cs.umn.edu/">IRVLab</a> and assisting in the field experiments      
												<br> <br>
												<i class="far fa-gem"></i> Additionally, I was a <a href="https://www.cs.umn.edu/news/cse-phd-students-named-doctoral-dissertation-fellows-1">DDF fellow</a> 
												and a <a href="http://www.dtc.umn.edu/fellowships/adcfellowship.php">DTC fellow</a> in the 2019-20 and 
												2015-16 academic years, respectively 
											</i></p>
										</article> 
										<article>
											<h3><i class="icon solid fas fa-book-open"></i> Graduate Teaching Assistant</h3>
											<h4><span style="font-weight:normal"> 
												<a href="https://www.cs.umn.edu/">Dept. of CSE, University of Minnesota</a> <br>
												</span>
											</h4>
											<p><i>
												<i class="fas fa-tasks"></i> I was involved in preparing and grading assignments of the following courses: <br>
												&nbsp; - <strong>Spring 2018:</strong> Introduction To Intelligent <br>
												&nbsp; &nbsp; Robotic Systems (CSci 5551), and <br> 
												&nbsp; - <strong>Fall 2017/16:</strong> Introduction to C/C++ <br>
												&nbsp; &nbsp; Programming (CSci 1113) <br>
												<i class="fas fa-users"></i> I also held office-hours and conducted occasional 
												lectures on relevant materials    
											</i></p>
										</article>
										<article>
											<div class="content">
												<h3><i class="icon solid fas fa-chalkboard-teacher"></i> Lecturer</h3>
												<h4><span style="font-weight:normal"> 
												<a href="http://www.uiu.ac.bd/">Dept. of CSE, United Int. University</a> <br>
												May 2012 - August 2015</a></span>
											</h4>
											<p><i>
												<i class="fas fa-bullhorn"></i> Major courses instructed by me: <br>
												&nbsp; - Introduction to Artificial Intelligence <br> 
												&nbsp; - Structured Programming Language <br> 
												&nbsp; - Microprocessors and Microcontrollers <br>
												&nbsp; - Data Communications and Networks <br>
												&nbsp; - Numerical Methods and Applications <br>
												&nbsp; - Electronic Devices and Circuits 
											</i></p>
											</div>
										</article>
									</div> 
								</section>
								<section, id="Edu">
									<header class="major">
										<h2>Education</h2>
									</header>
									<div class="posts">
										<article>
											<h3><i class="icon solid fas fa-school"></i> &nbsp; Ph. D., CS
											<span style="font-weight:normal"> [09/15 - Present]</span> </h3>
											<h4><span style="font-weight:normal"> <a href="https://www.cs.umn.edu/">University of Minnesota (UMN), Twin Cities, MN, USA</a></span> 
											</h4>
											<p>
												Supervised by Prof. <a href="http://www-users.cs.umn.edu/~junaed/">Junaed Sattar</a> <br> <br>
												<i><strong>Thesis topic:</strong> Machine Vision for Improved Human-Robot Cooperation in Adverse Underwater Conditions</i>
											</p>
										</article>
										<article>
											<h3><i class="icon solid fas fa-graduation-cap"></i> &nbsp; M. Sc., CSE
											<span style="font-weight:normal"> [05/12 - 02/15]</span> </h3>
											<h4><span style="font-weight:normal"> <a href="http://www.buet.ac.bd/">Bangladesh University of Eng. and Tech. (BUET), Dhaka, Bangladesh</a></span> 
											</h4>
											<p>
												Jointly supervised by Prof. <a href="http://cse.buet.ac.bd/faculty/facdetail.php?id=mmislam">Md. Monirul Islam</a> 
												and Prof. <a href="http://cse.buet.ac.bd/faculty/facdetail.php?id=razi">A. B. M. Alim Al Islam</a> <br> <br>
												<i><strong>Thesis:</strong> Intelligent Dynamic Spectrum Access for Cognitive Radio Networks (CRNs)</i>
											</p>
										</article>
										<article>
											<h3><i class="icon solid fas fa-graduation-cap"></i> &nbsp; B. Sc., CSE
											<span style="font-weight:normal"> [06/07 - 04/12]</span> </h3>
											<h4><span style="font-weight:normal"> <a href="http://www.buet.ac.bd/">Bangladesh University of Eng. and Tech. (BUET), Dhaka, Bangladesh</a></span> 
											</h4>
											<p>
												Supervised by Prof. <a href="http://cse.buet.ac.bd/faculty/facdetail.php?id=mmislam">Md. Monirul Islam</a> <br> <br>
												<i><strong>Thesis:</strong> Self-adaptive and Genetically Programmed Differential Evolution</i>
											</p>
										</article>
									</div>
								</section>


							<!-- Section -->
								<section, id="Proj">
								<hr class="major" />
									<header class="major">
										<h2>Selected Projects</h2>
									</header>
									<div class="mini-posts", id="VAM">
										<article>
											<h3>VAM: Visual Attention Modeling</h3>
											<div class="row">
												<div class="col-6 col-12-small">
													<p align="justify">
														<i>Where to look at?</i>&mdash; is an intruiging problem of computer vision that deals with finding interesting or <i>salient</i> pixels in an image/video. 
														For visually-guided robots, in particular, the <strong>Salient Object Detection (SOD)</strong> capability enables them to model spatial attention to eventually make important navigation decisions. We are working on a novel SOD model for real-time robotic applications; the initial results are promising and we wish to share exciting results soon. Till then, enjoy the <a href="https://youtu.be/9bZkp7q19f0">GANGNAM</a> saliency! 
													</p>
													
												</div>
												<div class="col-6 col-12-medium">
													<a href="#VAM" class="image"><img src="images/psy_gif.gif" alt=""/></a>
												</div>
											</div>
										</article>
									</div>
									<hr class="major" />

									<div class="mini-posts", id="SESR">
										<article>
											<h3>Deep SESR: Simultaneous Enhancement and Super-Resolution</h3>
											<p>
												<a href="https://roboticsconference.org/program/papers/18/">Paper (RSS 2020)</a> &nbsp;
												<a href="https://arxiv.org/pdf/2002.01155.pdf">ArXiv</a> &nbsp; 
												<a href="files/islam2020sesr.txt">Bibliography</a> &nbsp;
												<a href="https://github.com/xahidbuffon/Deep-SESR">GitHub</a> &nbsp;
												<a href="http://irvlab.cs.umn.edu/resources/ufo-120-dataset">UFO-120 Dataset</a> &nbsp;
												<a href="https://youtu.be/8zBdFxaK4Os">Spotlight Talk</a>
											</p>
											<div class="row">
												<div class="col-3 col-12-small">
													<p align="justify">
													In this project, we introduce the <strong>SESR</strong> (simultaneous enhancement and super-resolution) problem and provide an efficient solution for underwater imagery. Specifically, we present <strong>Deep SESR</strong>, a residual-in-residual network-based model that learn to restore perceptual image qualities for up to 4x higher spatial resolution. 
													</p>
												</div>
												<div class="col-9 col-12-medium">
													<a href="#SESR" class="image"><img src="images/sesr_1.jpg" alt="" /></a>
											<a href="#SESR" class="image"><img src="images/sesr_2.jpg" alt="" /></a>
												</div>
											</div>
											<p align="justify">
											To supervise the large-scale training, we formulate a multi-modal objective function that addresses the chrominance-specific underwater color degradation, lack of image sharpness, and loss in high-level feature representation. It is also supervised to learn salient foreground regions in the image, which in turn guides the network to learn global contrast enhancement. Check out our <a href="https://youtu.be/8zBdFxaK4Os">Spotlight Talk</a> for more details! 
											</p>

											<div class="row">
												<div class="col-7 col-12-medium">
													<iframe width="640" height="360" src="https://www.youtube.com/embed/wEkTu2CPW-g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
												</div>
												<div class="col-5 col-12-small">
												<p align="justify">
												Moreover, we present <strong>UFO-120 Dataset</strong>, the first dataset to facilitate large-scale SESR learning; it contains over 1500 training samples and a benchmark test set of 120 samples.
												By thorough experimental evaluation on UFO-120 and several other standard datasets, we demonstrate that Deep SESR outperforms the existing solutions for underwater image enhancement and super-resolution. We also validate its generalization performance on several test cases that include underwater images with diverse spectral and spatial degradation levels, and also terrestrial images with unseen natural objects. Please refer to the <a href="https://roboticsconference.org/program/papers/18/">paper</a> for detailed demonstrations and results. 
											    </p>
												</div>
											</div>  
										</article>
									</div>
									<hr class="major" />

									<div class="mini-posts", id="FUNIE">
										<article>
											<h3>FUNIE-GAN: Fast Underwater Image Enhancement for Improved Perception</h3>
											<p>
												<a href="https://ieeexplore.ieee.org/document/9001231">Paper (RA-L)</a> &nbsp;
												<a href="https://arxiv.org/pdf/1903.09766.pdf">ArXiv</a> &nbsp; 
												<a href="files/islam2020fast.txt">Bibliography</a> &nbsp;
												<a href="https://github.com/xahidbuffon/FUnIE-GAN">GitHub</a> &nbsp;
												<a href="http://irvlab.cs.umn.edu/resources/euvp-dataset">EUVP Dataset</a>
											</p>
											<div class="row">
												<div class="col-6 col-12-medium">
													<a href="#FUNIE" class="image"><img src="images/funie_gif1.gif" alt="" /></a>
												</div>
												<div class="col-6 col-12-small">
													<p align="justify">
													In this project, we design a fully-convolutional conditional GAN-based model for fast underwater image enhancement: <strong>FUnIE-GAN</strong>. We supervise its adversarial training by formulating an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. <br> <br>
													We also present <strong>EUVP</strong>, a large-scale dataset of a paired and an unpaired collection of underwater images (of poor and good quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. The dataset and relevant information can be found <a href="http://irvlab.cs.umn.edu/resources/euvp-dataset">here</a>.    
													</p>
												</div>
											</div>

											<p align="justify">
											By thorough experiments, we demonstrate that FUnIE-GAN can learn to enhance perceptual image quality from both paired and unpaired training. More importantly, the enhanced images significantly boost the performance of several underwater visual perception tasks such as object detection, human pose estimation, and saliency prediction. Please refer to the <a href="https://arxiv.org/pdf/1903.09766.pdf">paper</a> for these experimental analysis and detailed results.
											</p>

											<div class="row">
												<div class="col-6 col-12-small">
													<p align="justify">
													In addition to providing state-of-the-art enhancement performance, FUnIE-GAN offers <strong>148 FPS</strong> inference rate on Nvidia GTX 1080, <strong>48 FPS</strong> on Nvidia Jetson AGX Xavier, and over <strong>25 FPS</strong> on Nvidia Jetson TX2. Such fast run-times, particularly on the single-board platforms, makes it ideal for real-time use in robotic applications. Please check out <a href="https://github.com/xahidbuffon/FUnIE-GAN">this repository</a> for the FUnIE-GAN models and associated training pipelines. <br> <br>

													<i>
														<strong><i class="fas fa-truck-monster"></i> &nbsp; Latest:</strong> FUnIE-GAN is now running on our <a href="http://irvlab.cs.umn.edu/minnebot">Aqua-8 MinneBot</a><br>
														<i class="fas fa-truck-monster"></i> &nbsp; ...and soon to be ported on the <a href="http://irvlab.cs.umn.edu/other-projects/loco-auv">LoCO AUV</a>
													</i>

													</p>
													
												</div>
												<div class="col-6 col-12-medium">
													<a href="#FUNIE" class="image"><img src="images/funie_1b.jpg" alt="" /></a> 
												</div>
											</div>
										</article>
									</div>
									<hr class="major" />

									<div class="mini-posts", id="SRDRM">
										<article>
											<h3>SRDRM: Underwater Image Super-Resolution using Deep Residual Multipliers</h3>
											<p>
												<a href="https://arxiv.org/pdf/1909.09437.pdf">ArXiv</a> &nbsp; 
												<a href="files/islam2020srdrm.txt">Bibliography</a> &nbsp;
												<a href="https://github.com/xahidbuffon/SRDRM">GitHub</a> &nbsp;
												<a href="http://irvlab.cs.umn.edu/resources/usr-248-dataset">USR-248 Dataset</a> 
											</p>
											<div class="row">
												<div class="col-7 col-12-medium">
													<a href="#SRDRM" class="image"><img src="images/srdrm_1b.jpg" alt="" /></a>
												</div>
												<div class="col-5 col-12-small">
													<p align="justify">
													Single Image Super-Resolution (SISR) allows zooming-in interesting image regions for detailed visual perception. 
													In this project, we design a deep residual network-based generative model for 2x, 4x, and 8x SISR of underwater imagery. We provide a generative and an adversarial training pipeline for the model, which we refer to as <strong>SRDRM</strong> and <strong>SRDRM-GAN</strong>, respectively.  <br> <br>
													<i><strong><i class="fas fa-check"></i> &nbsp; Update: </strong> accepted for publication at the <a href="http://icra2020.org/">ICRA-2020</a> </i>  
													</p>
												</div>
											</div>

											<p align="justify">
											In our implementation, both SRDRM and SRDM-GAN learn to generate 640 x 480 images from respective inputs of size 320 x 240, 160 x 120, or 80 x 60. 
											For the supervised training, we use natural underwater images which we collected by several oceanic explorations and field trials. We also included images from publicly available online media resources such as <i class="fab fa-youtube"></i> YouTube and <i class="fab fa-flickr"></i> Flickr. We released this compiled dataset named <strong>USR-248</strong> for academic research purposes <a href="http://irvlab.cs.umn.edu/resources/usr-248-dataset">here</a>.
											</p>

											<a href="#SRDRM" class="image"><img src="images/srdrm_3a.jpg" alt="" /></a>

											<p align="justify">
												We validate the effectiveness of SRDRM and SRDRM-GAN through qualitative and quantitative experiments and compare the results with several state-of-the-art models' performances. 
												We also analyze their practical feasibility for applications such as scene understanding and attention modeling in noisy visual conditions. Please refer to the <a href="https://arxiv.org/pdf/1909.09437.pdf">paper</a> for detailed results and performance analysis.
											</p>
											
											
										</article>
									</div>
									<hr class="major" />

									<div class="mini-posts", id="R2R">
										<article>
											<h3>R2R-OpenPose: Robot-to-robot Relative Pose Estimation from Human Body-Pose</h3>
											<p>
												<a href="https://arxiv.org/pdf/1903.00820.pdf">ArXiv</a> &nbsp; 
												<a href="files/islam2019robot.txt">Bibliography</a> 
											</p>
											<div class="row">
												<div class="col-5 col-12-medium">
													<a href="#R2R" class="image"><img src="images/r2r_1.jpg" alt="" /></a>
												</div>
												<div class="col-7 col-12-small">
													<p align="justify">
													In this project, we propose a method to determine the 3D relative pose of pairs of communicating robots by using human pose-based key-points as correspondences. We adopt a <i>leader-follower</i> framework, where at first, the leader robot visually detects and triangulates the key-points using the state-of-the-art pose detector named <strong>OpenPose</strong>. Afterward, the follower robots match the corresponding 2D projections on their respective calibrated cameras and find their relative poses by solving the perspective-n-point (<strong>PnP</strong>) problem. 
													<br> <br>
													In the proposed method, we design an efficient <strong>person re-identification</strong> technique for associating the mutually visible humans in the scene. Additionally, we present an iterative optimization algorithm to refine the associated key-points based on their local structural properties in the image space. 
													</p>
												</div>
											</div>

											<div class="row">
												<div class="col-4 col-12-medium">
													<p align="justify">
													We demonstrate that the proposed refinement processes are essential to establish accurate key-point correspondences across viewpoints. Furthermore, we evaluate the performance of the end-to-end pose estimation system through several experiments conducted in terrestrial and underwater environments. 
												</p> 
												</div>
												<div class="col-8 col-12-small">
													<p align="justify">
													<a href="#R2R" class="image"><img src="images/r2r_2.gif" alt="" /></a>
													</p>
												</div>
											</div>
											<p align="justify">
												Finally, we discuss the relevant operational challenges of this approach and analyze its feasibility for multi-robot cooperative systems in human-dominated social settings and feature-deprived environments such as underwater. Please refer to the <a href="https://arxiv.org/pdf/1903.00820.pdf">paper</a> for further details. 
											</p> 

										</article>
									</div>
									<hr class="major" />

									<div class="mini-posts", id="PFollow">
										<article>
											<h3>Person Following by Autonomous Robots: A Categorical Overview</h3>
											<p>
												<a href="https://journals.sagepub.com/doi/10.1177/0278364919881683">Paper (IJRR)</a> &nbsp;
												<a href="https://arxiv.org/pdf/1803.08202.pdf">ArXiv</a> &nbsp; 
												<a href="files/islam2019person.txt">Bibliography</a> 
											</p>
											<a href="#PFollow" class="image"><img src="images/ppic.jpg" alt="" /></a>
											<p align="justify">
											A wide range of human-robot collaborative applications in diverse domains such as manufacturing, health care, the entertainment industry, and social interactions, require an autonomous robot to follow its human companion. Different working environments and applications pose diverse challenges by adding constraints on the choice of sensors, the degree of autonomy, and dynamics of a person-following robot. Researchers have addressed these challenges in many ways and contributed to the development of a large body of literature. 
											This paper provides a comprehensive overview of the literature by categorizing different aspects of person-following by autonomous robots. Also, the corresponding operational challenges are identified based on various design choices for <strong>ground</strong>, <strong>underwater</strong>, and <strong>aerial</strong> scenarios. In addition, state-of-the-art methods for <strong>perception</strong>, <strong>planning</strong>, <strong>control</strong>, and <strong>interaction</strong> are elaborately discussed and their applicability in varied operational scenarios are presented. Then, some of the prominent methods are qualitatively compared, corresponding practicalities are illustrated, and their feasibility is analyzed for various usecases. Furthermore, several <strong>prospective application areas</strong> are identified, and <strong>open problems</strong> are highlighted for future research.
											</p>
										</article>
									</div>
									<hr class="major" />

									<div class="mini-posts", id="DDD">
										<article>
											<h3>DDD: Balancing Robustness and Efficiency Deep Diver Detection</h3>
											<p>
												<a href="https://ieeexplore.ieee.org/document/8543168">Paper (RA-L)</a> &nbsp;
												<a href="https://arxiv.org/pdf/1809.06849.pdf">ArXiv</a> &nbsp;
												<a href="https://github.com/xahidbuffon/Deep-Diver-Following">Code</a> &nbsp; 
												<a href="files/islam2018towards.txt">Bibliography</a> 
											</p>
											<div class="row">
												<div class="col-3 col-12-medium">
													<a href="#DDD" class="image"><img src="images/ddd_pool.gif" alt="" /></a>
												</div>
												<div class="col-3 col-12-medium">
													<a href="#DDD" class="image"><img src="images/ddd_bbd.gif" alt="" /></a>
												</div>
												<div class="col-6 col-12-small">
													<p align="justify">
													In this project, we explore the design and development of a class of robust diver-following algorithms for autonomous underwater robots. By considering the operational challenges for underwater visual tracking in diverse real-world settings, we formulate a set of desired features of a generic diver following algorithm. We attempt to accommodate these features and maximize general tracking performance by exploiting the SOTA deep object detection models: <strong>Faster R-CNN (Inception V2)</strong>, <strong>YOLO V2</strong>, <strong>Tiny YOLO</strong>, and <strong>SSD (MobileNet V2)</strong>. 
													</p>
												</div>
											</div>
												<p align="justify">
												Subsequently, we design an architecturally simple <strong>CNN-based diver-detection model</strong> that is much faster than the SOTA deep models yet provides comparable detection performance. Each building block of the proposed model was fine-tuned in order to balance the trade-off between robustness and efficiency for a single-board setting under real-time constraints. 
												We also validated its tracking performance and general applicability through numerous field experiments in pools and oceans. This detection model is used in the diver-following module of our <a href="http://irvlab.cs.umn.edu/minnebot">Aqua-8 MinneBot</a>. 
												</p>
										</article>
									</div>
									<hr class="major" />

									<div class="mini-posts", id="RoboChatGest">
										<article>
											<h3>Robo-Chat-Gest: Hand Gesture-based Robot Control and Reconfiguration</h3>
											<p>
												<a href="https://ieeexplore.ieee.org/document/8461197">Paper-1 (ICRA 2018)</a> &nbsp;
												<a href="files/islam2018dynamic.txt">Bibliography-1</a> &nbsp;
												<a href="https://onlinelibrary.wiley.com/doi/full/10.1002/rob.21837">Paper-2 (JFR)</a> &nbsp;
												<a href="files/islam2018understanding.txt">Bibliography-2</a> 
											</p>

											<div class="row">
												<div class="col-4 col-12-medium">
													<a href="#RoboChatGest" class="image"><img src="images/RoboChatGest_pool.gif" alt="" /></a>
												</div>
												<div class="col-4 col-12-medium">
													<a href="#RoboChatGest" class="image"><img src="images/RoboChatGest_bbd.gif" alt="" /></a>
												</div>
												<div class="col-4 col-12-small">
													<p align="justify">
													<strong>Robo-Chat-Gest</strong> introduces a real-time robot programming and parameter reconfiguration method for autonomous underwater robots using a set of intuitive and meaningful hand gestures. It is a syntactically simple framework and computationally more efficient than other existing grammar-based approaches.  
												</div>
											</div>
											<p align="justify">
												The major components of Robo-Chat-Gest are: (a) a simple set of hand-gestures to instruction mapping rules, (b) a region selection mechanism to detect prospective hand gestures in the image-space, (c) a CNN-based model for robust hand gesture recognition, and (d) a Finite-State Machine (FSM) to efficiently decode complete instructions from the sequence of gestures. 
												The key aspect of this framework is that it can be easily adopted by divers for communicating simple instructions to underwater robots without using artificial tags such as fiducial markers or requiring them to memorize a potentially complex set of language rules. 
												Please refer to <a href="https://ieeexplore.ieee.org/document/8461197">this paper</a> for the design and implementation details of Robo-Chat-Gest; moreover, its practical utility and usability benefits are analysed in <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/rob.21837">this paper</a>.   
											</p>
										</article>
									</div>
								</section>

								<section, id="Colab">
								<hr class="major" />
									<header class="major">
										<h2>Collaborations</h2>
									</header>
									<div class="mini-posts", id="UGAN">
										<article>
											<h3>UGAN: Underwater Image Enhancement by Generative Adversarial Networks</h3>
											<div class="row">
												<div class="col-8 col-12-small">
													<p align="justify">
													I worked with my former colleague <a href="https://cameronfabbri.github.io/">Cameron Fabbri</a> on his <a href="https://ieeexplore.ieee.org/document/8460552">UGAN paper  (ICRA 2018)</a>, which introduced a method to improve the quality of underwater imagery using Generative Adversarial Networks (GANs). The objective was to enhance noisy input images to boost the performance of various vision-based tasks such as detection and tracking. I was involved in quantifying these performance margins and in relevant experimental evaluations. 
													The proposed UGAN model and the associated training and evaluation pipelines can be found in <a href="https://github.com/cameronfabbri/Underwater-Color-Correction">this GitHub repository</a>.
													</p>
												</div>
												<div class="col-4 col-12-medium">
													<a href="#UGAN" class="image"><img src="images/ugan.png" alt=""/></a>
												</div>
											</div>
										</article>
									</div>
									<hr class="major" />

									<div class="mini-posts", id="CONVOY">
										<article>
											<h3>Underwater Multi-Robot Convoying using Visual Tracking by Detection</h3>
											<div class="row">
											<div class="col-8 col-12-small">
												<p align="justify">
												I collaborated with my peers at the <a href="http://www.cim.mcgill.ca/~mrl/index.html">Mobile Robotics Lab @McGill University</a> on this project lead by <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a> (now an Assistant Professor <a href="https://web.cs.toronto.edu/">@UToronto</a>). 
												This project presented a robust multi-robot convoying approach relying on visual detection of the leading agent, thus enabling target following in unstructured 3D environments. 
												I was involved in the evaluation of <a href="http://ieeexplore.ieee.org/document/7989516/">MDPM tracker</a>), which was used in the empirical performance comparison of other contemporary visual trackers.    
												Please check out the <a href="https://ieeexplore.ieee.org/document/8206280/">the paper (IROS 2017)</a> and <a href="http://www.cim.mcgill.ca/~mrl/robot_tracking/">project page</a> for more information. 
												</p>
											</div>
											<div class="col-4 col-12-medium">
													<a href="#CONVOY" class="image"><img src="images/convoy.png" alt=""/></a>
											</div>

											</div>
										</article>
									</div>
									<hr class="major" />

									<div class="mini-posts", id="Mentor">
										<article>
											<div class="row">
											<div class="col-5 col-12-medium">
												<h3><i class="fas fa-user-friends"></i> Other Collaborators</h3>
												<p align="justify">
													I regulary collaborate with the current Ph.D. students at the IRVLAB. In particular, I heavily interacted with <a href="http://irvlab.cs.umn.edu/people/chelsey-edge">Chelsey Edge</a>, <a href="https://sites.google.com/view/sadman-sakib-enan/home">Sadman Sakib Enan</a>, <a href="http://irvlab.cs.umn.edu/people/michael-fulton">Michael Fulton</a>, and <a href="https://jungseokhong.github.io/">Jungseok Hong</a> on several <a href="Proj">projects</a> in the last four years. I am currently working with <a href="https://www.linkedin.com/in/jiawei-mo/">Jiawei Mo</a> and <a href="http://irvlab.cs.umn.edu/people/karin-de-langis">Karin de Langis</a> on separate projects. Also, <a href="https://www.linkedin.com/in/ruobing-wang-054562157/">Ruobing Wang</a> is working with me on the <a href="https://www.linkedin.com/in/ruobing-wang-054562157/">VAM</a> project.
												</p>
											</div>
											<div class="col-7 col-12-small">
											   <h3><i class="fas fa-users-cog"></i> UG Students Whom I Mentored and Worked With</h3>
												<p>
												    &nbsp;  &nbsp; - <a href="https://www.linkedin.com/in/peigen-luo-30ba21147/">Peigen Luo</a> (on <a href="#SESR">SESR</a> and <a href="#SRDRM">SRDRM</a> project); he is now a graduate student at CMU <br>
													&nbsp;  &nbsp; - Yuyang Xiao (on <a href="https://github.com/xahidbuffon/SUIM-Net">SUIM</a> project); he is now a graduate student at UIUC <br>
													&nbsp;  &nbsp; - <a href="https://www.xiayouya.com/">Youya Xia</a> (on <a href="#FUNIE">FUNIE-GAN</a> project); she is now a PhD student at Cornell <br>
													&nbsp;  &nbsp; - <a href="https://www.linkedin.com/in/marc-ho-96b530193/">Marc Ho</a> (on <a href="#RoboChatGest">Robo-Chat-Gest</a> project); he is now working at Optum <br>
													&nbsp;  &nbsp; - <a href="http://irvlab.cs.umn.edu/people/muntaqim-mehtaz">Muntaqim Mehtaz</a> and <a href="http://irvlab.cs.umn.edu/people/christopher-morse">Christopher Morse</a> (on <a href="https://github.com/xahidbuffon/SUIM-Net">SUIM</a> project); they are current <br> 
													&nbsp;  &nbsp; &nbsp; students at the UMN
												</p>
											</div>
											</div>
										</article>
									</div>
								</section>

								<!-- Section -->
								<section, id="Pub">
								<hr class="major" />
									<header class="major">
										<h2>List of Publications</h2>
									</header>
									<div class="mini-posts">
										<article> 
										<h3>2019-2020</h3>
										<ol>
											<li> <strong>M. J. Islam</strong>, P. Luo, and J. Sattar.   
												<i><a href= "https://roboticsconference.org/program/papers/18/">Simultaneous Enhancement and Super-Resolution of Underwater Imagery for Improved Visual Perception</a></i>. 
												Robotics: Science and Systems (<a href= "https://roboticsconference.org/">RSS</a>), 2020. &nbsp;
												[<a href= "https://github.com/xahidbuffon/Deep-SESR"><span style="color: blue;">code</span></a> &nbsp; 
												<a href= "http://irvlab.cs.umn.edu/resources/ufo-120-dataset"><span style="color: blue;">data</span></a> &nbsp; 
												<a href= "files/islam2020sesr.txt"><span style="color: blue;">bib</span></a>]
											</li>
									        <li> <strong>M. J. Islam</strong>, Y. Xia, and J. Sattar.  
									        	<i><a href= "https://ieeexplore.ieee.org/document/9001231">Fast Underwater Image Enhancement for Improved Visual Perception</a></i>. 
									        	IEEE Robotics and Automation Letters (<a href= "http://www.ieee-ras.org/publications/ra-l">RA-L*</a>), vol. 5, no. 2, pp. 3227-3234, 2020, doi: 10.1109/LRA.2020.2974710. &nbsp;
												[<a href= "https://github.com/xahidbuffon/FUnIE-GAN"><span style="color: blue;">code</span></a> &nbsp; 
												<a href= "http://irvlab.cs.umn.edu/resources/euvp-dataset"><span style="color: blue;">data</span></a> &nbsp; 
												<a href= "files/islam2020fast.txt"><span style="color: blue;">bib</span></a>] &nbsp; <em>*<span style="color: green;">Impact Factor: 3.61</span></em>
									        </li> 
									        <li> <strong>M. J. Islam</strong>  S. S. Enan, P. Luo, and J. Sattar.  
									        	<i><a href= "https://arxiv.org/abs/1909.09437">Underwater Image Super-Resolution using Deep Residual Multipliers</a></i>.  
									        	To appear at the IEEE Int. Conf. on Robotics and Automation (<a href= "http://www.icra2020.org/">ICRA</a>), 2020, Virtual. &nbsp;
												[<a href= "https://github.com/xahidbuffon/SRDRM"><span style="color: blue;">code</span></a> &nbsp; 
												<a href= "http://irvlab.cs.umn.edu/resources/usr-248-dataset"><span style="color: blue;">data</span></a> &nbsp; 
												<a href= "files/islam2020srdrm.txt"><span style="color: blue;">bib</span></a>]
									        </li> 
									        <li> <strong>M. J. Islam</strong>  C. Edge, Y. Xiao, P. Luo, M. Mehtaz, C. Morse, S. S. Enan, and J. Sattar.  
									        	<i><a href= "https://arxiv.org/abs/2004.01241">Semantic Segmentation of Underwater Imagery: Dataset and Benchmark.</a></i>.  
									        	To appear at the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (<a href= "https://www.iros2020.org/">IROS</a>), 2020, Virtual. &nbsp;
												[<a href= "https://github.com/xahidbuffon/SUIM-Net"><span style="color: blue;">code</span></a> &nbsp; 
												<a href= "http://irvlab.cs.umn.edu/resources/suim-dataset"><span style="color: blue;">data</span></a> &nbsp; 
												<a href= "files/islam2020suim.txt"><span style="color: blue;">bib</span></a>]
									       	</li> 
									        <li> <strong>M. J. Islam</strong>, J. Hong, and J. Sattar.  
									        	<i><a href= "https://journals.sagepub.com/doi/10.1177/0278364919881683">Person Following by Autonomous Robots: A Categorical Overview</a></i>. 
									        	International Journal of Robotics Research (<a href= "http://journals.sagepub.com/home/ijr">IJRR*</a>), vol. 38, no. 14, pp. 1581-1618, 2019, doi: 10.1177/0278364919881683. 
									        	[<a href= "files/islam2019person.txt"><span style="color: blue;">bib</span></a>] &nbsp; 
									        	<em>*<span style="color: green;">Impact Factor: 6.134</span></em> 
									        </li>
									    </ol>
									    <h3>2017-2018</h3>
									    <ol> 
									        <li> <strong>M. J. Islam</strong>, M. Fulton, and J. Sattar.  
									        	<i><a href= "https://ieeexplore.ieee.org/document/8543168">Towards a Generic Diver-Following Algorithm: Balancing Robustness and Efficiency in Deep Visual Detection</a></i>. 
									        		IEEE Robotics and Automation Letters (<a href= "http://www.ieee-ras.org/publications/ra-l">RA-L</a>),  vol. 4, no. 1, pp. 113-120, 2018, doi: 10.1109/LRA.2018.2882856.
									        		[<a href= "https://github.com/xahidbuffon/deep-diver-following"><span style="color: blue;">code</span></a>, &nbsp;
									        		<a href= "files/islam2018towards.txt"><span style="color: blue;">bib</span></a>] &nbsp; 
									        		<em>*<span style="color: green;">Presented at the <a href= "http://icra2019.org">ICRA 2019</a></span></em>  
									        	</li> 
									        <li> <strong>M. J. Islam</strong>, M. Ho, and J. Sattar.  
									        	<i><a href= "https://onlinelibrary.wiley.com/doi/full/10.1002/rob.21837">Understanding Human Motion and Gestures for Underwater Human-Robot Collaboration</a></i>. 
									        	Journal of Field Robotics (<a href= "http://www.journalfieldrobotics.org/Home.html">JFR*</a>), 2018, doi: 10.1002/rob.21837. 
									        	[<a href= "files/islam2018understanding.txt"><span style="color: blue;">bib</span></a>] &nbsp; 
									        	<em>*<span style="color: green;">Impact Factor: 4.345</span></em>
									        </li> 
									        <li> <strong>M. J. Islam</strong>, M. Ho, and J. Sattar.  
									        	<i><a href= "https://ieeexplore.ieee.org/document/8461197">Dynamic Reconfiguration of Mission Parameters in Underwater Human-Robot Collaboration</a></i>. 
									        	IEEE Int. Conf. on Robotics and Automation (<a href= "http://www.icra2018.org/">ICRA</a>), 2018, pp. 1-8, Brisbane, Australia. 
									        	[<a href= "https://github.com/xahidbuffon/RoboChatGest"><span style="color: blue;">code</span></a>, &nbsp;
									        	<a href= "files/islam2018dynamic.txt"><span style="color: blue;">bib</span></a>] 
									        </li> 
									    </ol>
									    <h3>2016-2017</h3>
									    <ol> 
									        <li> <strong>M. J. Islam</strong> and J. Sattar.  
									        	<i><a href= "http://ieeexplore.ieee.org/document/7989516/">Mixed-domain Biological Motion Tracking for Underwater Human-Robot Interaction</a></i>.  
									        	IEEE Int. Conf. on Robotics and Automation (<a href= "http://www.icra2017.org/">ICRA</a>), 2017, pp. 4457-4464, Singapore. 
									        	[<a href= "files/islam2017mixed.txt"><span style="color: blue;">bib</span></a>]
									        </li> 
									        <li> C. Fabbri, <strong>M. J. Islam</strong>, and J. Sattar.  
									        	<i><a href= "https://ieeexplore.ieee.org/document/8460552">Enhancing Underwater Imagery using Generative Adversarial Networks</a></i>. 
									        	IEEE Int. Conf. on Robotics and Automation (<a href= "http://www.icra2018.org/">ICRA</a>), 2018, pp. 7159-7165, Brisbane, Australia. 
									        </li>
									        <li>F. Shkurti, W. Chang, P. Henderson, <strong>M. J. Islam</strong>, J. C. G. Higuera, J. Li, T. Manderson, A. Xu, G. Dudek, and J. Sattar. 
									        	<i><a href= "https://ieeexplore.ieee.org/document/8206280/">Underwater Multi-Robot Convoying using Visual Tracking by Detection</a></i>.  
									        	IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (<a href= "http://www.iros2017.org/">IROS</a>), 2017, pp. 4189-4196, Vancouver, Canada. 
									        </li>
									        <li> A. B. M. A. A. Islam, <strong>M. J. Islam</strong>, N. Nurain, and V. Raghunathan.  
									        	<i><a href= "http://ieeexplore.ieee.org/document/7360096/">Channel Assignment Techniques for Multi-Radio Wireless Mesh Networks: A Survey</a></i>. 
									        	IEEE Communications Surveys and Tutorials (<a href= "https://www.comsoc.org/cst">IEEE ComST*</a>), 2016, pp. 988-1017, doi: 10.1109/COMST.2015.2510164. 
									        	<em>*<span style="color: green;">Impact Factor: 22.973</span></em> 
									        </li> 
									        
										</ol>
										</article>
									</div>
								</section>
								<hr class="major" />

								<section, id="Indus">
									<header class="major">
									<h2>Industry Experience</h2>
									</header>
									<div class="posts"> 
										<article>
											<img src="images/Q_logo.jpg_large" alt="" class="image" height="75"/> 
											<strong>2019 Summer Internship, R&D</strong> <br>
													Qualcomm Technologies, Inc. <br> 
													Santa Clara, CA, USA <br> <br>
											<p>I worked with the <strong>Glance team</strong> on a low-power computer vision project.</p>
										</article>
										<article>
											<img src="images/3M_logo.png" alt="" class="image" height="75"/> 
											<strong>2018 Summer Internship, R&D</strong> <br>
													Corporate Research Systems Lab, 3M <br> 
													Maplewood, MN, USA <br> <br>
											<p>I worked at the <strong>AI laboratory</strong> on a couple of exciting projects.</p>
										</article>
										<article>
											<i><h4><i class="fas fa-highlighter"></i> Exposure</h4>
											<p align="justify">
											  On both the internships, I was involved with the design and customization 
											  of various computer vision and machine learning modules for real systems. 
											  I enjoyed the experience of learning how research work or white-board ideas 
											  get transformed into concrete features of actual products.
											</p> </i>
										</article>
									</div>
								</section>
								<section, id="Skill">
									<header class="major">
									<h2>Skill Sets</h2>
									</header>
									<div class="posts"> 
										<article>
											<h4><i class="fas fa-code"></i> Programming Labguages</h4> 
												&nbsp; <i class="fab fa-python"></i> Python, &nbsp; <i class="fas fa-coffee"></i> C++/C <br>
												&nbsp; Java, MATLAB, Unix Shell <br> <br>

											<h4><i class="fas fa-magic"></i> Operating Systems</h4>
												&nbsp; <i class="fab fa-ubuntu"></i> Linux (Ubuntu), &nbsp; <i class="fab fa-windows"></i> Windows
										</article>
										<article>
											<h4><i class="fas fa-tools"></i> Libraries and Tool-kits</h4>
											<ul>
												<li> OpenCV 3.0 </li>
												<li> TensorFlow (1.11+ and 2.0) </li>
												<li> Keras 2.2.0+ </li>
												<li> PyTorch 1.5.1 </li>
												<li> ROS: Kinetic and Melodic </li>
											</ul>
										</article>
										<article>
											<h4><i class="fas fa-microchip"></i> Embedded AI devices</h4>
												&nbsp; Nvidia Jetson Xavier, TX2, Nano <br>
												&nbsp; Google Coral Edge TPU <br> <br>
											<h4><i class="fas fa-robot"></i> Robotic platforms</h4>
												&nbsp; AQUA 8, TurtleBot 2, OpenROV 
											
										</article>
									</div>
								</section>

								<!-- Section -->
								<section, id="Awa">
								<hr class="major" />
									<header class="major">
										<h2>Selected Awards and Achievements</h2>
									</header>
									<div class="mini-posts">
										<article>
										<ul, style="line-height:0.6;">
										<li> <a href= "https://grad.umn.edu/funding/current-students/apply-through-program/doctoral-dissertation-fellowship">
											Doctoral Dissertation Fellowship (DDF)</a> award (2019-20), University of Minnesota. 
											[<a href= "https://www.cs.umn.edu/news/cse-phd-students-named-doctoral-dissertation-fellows-1">news</a>]
										</li> &nbsp
							            <li> <a href= "https://www.icra2019.org/registration/ras-travel-grant">RAS Travel Grant</a>  for participating  
							            	<a href= "https://icra2019.org/">ICRA 2019</a> in Montreal, Canada. 
							            </li> &nbsp
							            <li> <a href= "https://www.iros2017.org/registration-travel/travel-awards">IEEE/RSJ Travel Grant</a>  
							            	for participating  <a href= "http://www.iros2017.org/">IROS 2017</a> in Vancouver, Canada.
							            </li> &nbsp
							            <li> <a href= "http://www.dtc.umn.edu/fellowships/adcfellowship.php">ADC Graduate Fellowship in Wireless 
							            	and Networking Technology</a>  (2015-16), University of Minnesota. 
							            </li> &nbsp
							            <li> <strong>Runner-up</strong> (Team: BUET Skull).  
							            	<a href= "http://www.techfest.org/irc">International Robotics Challenge, IRC Grand Finale</a>, 
							            	Techfest 2012-13, IIT-Bombay, India.  
							            </li> &nbsp
							            <li> <strong>Champion</strong> (Team: BUET Skull).  
							            	<a href= "http://www.ieee.org.bd/international-robotics-challenge-bangladesh/"> 
							            	Bangladesh Regional of IRC</a>, 2012-13, IEEE Student Branch, BUET, Bangladesh. 
										</li> </ul>
										</article>
									</div>
								</section>

								<!-- Section -->
								<section, id="Extra">
								<hr class="major" />
									<header class="major">
										<h2>Community Work and Extracurriculars</h2>
									</header>
									<div class="posts">
									<article>
											<h4><i class="far fa-edit"></i> Int. Conference Reviewer</h4> 
												&nbsp; &nbsp; &nbsp; - ICCV 2019, ICRA 2020-16 <br>
												&nbsp; &nbsp; &nbsp; - IROS 2020-16, CRV 2019 <br> <br>

											<h4><i class="fas fa-edit"></i> Journal Referee</h4> 
												&nbsp; &nbsp; &nbsp; - IEEE RA-L, SPIC (Elsevier) <br> 
												&nbsp; &nbsp; &nbsp; - IEEE Signal Processing Letters <br> 
												&nbsp; &nbsp; &nbsp; - IEEE Tran. on Industrial Electronics
										</article>
										<article>
											<h4><i class="fas fa-dumbbell"></i> I <del>Play</del> Live Cricket</h4>
											<a href="pages/cricket.html" class="image"><img src="images/me_c1.jpg" height=240 alt="" /></a>
										</article>
										<article>
											<h4><i class="fas fa-camera"></i> And I <del>Take</del> Make Photos</h4>
											<a href="pages/lens.html" class="image"><img src="images/me_p1.jpg" height=240 alt="" /></a>
										</article>
									</div>
									<hr class="major" />
								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Pointers</h2>
									</header>
									<ul>
										<li><a href="index.html">Home</a></li>
										<li><a href="#Enrol">Academic Enrollments</a></li>
										<li>
											<span class="opener">Research Highlights</span>
											<ul>
												<li><a href="#VAM"><strong>VAM:</strong> Visual Attention Modeling</a></li>
												<li><a href="#SESR"><strong>Deep SESR:</strong> Simultaneous Enhancement and Super-Resolution</a></li>
												<li><a href="#FUNIE"><strong>FUNIE-GAN:</strong> Fast Underwater Image Enhancement for Improved Perception</a></li>
												<li><a href="#SRDRM"><strong>SRDRM:</strong> Deep Residual Multipliers for Underwater Image Super-Resolution</a></li>
												<li><a href="#R2R"><strong>R2R-OpenPose:</strong> Robot-to-robot Relative Pose Estimation from Human Body-Pose</a></li>
												<li><a href="#PFollow"><strong>A Holistic Review:</strong> Person Following by Autonomous Robots</a></li>
												<li><a href="#DDD"><strong>DDD:</strong> Balancing Robustness and Efficiency Deep Diver Detection</a></li>
												<li><a href="#RoboChatGest"><strong>Robo-Chat-Gest:</strong> Hand Gesture-based Robot Control and Reconfiguration</a></li>
											</ul>
										</li>
										<!-- <li>
											<span class="opener">Contributed Datasets</span>
											<ul>
												<li><a href="http://irvlab.cs.umn.edu/resources/suim-dataset"><strong>SUIM:</strong> 
												        Dataset for Semantic Segmentation of Underwater Imagery</a></li>
												<li><a href="http://irvlab.cs.umn.edu/resources/ufo-120-dataset"><strong>UFO-120:</strong> 
												        Dataset for Underwater Image Enhancement and Super-Resolution</a></li>
												<li><a href="http://irvlab.cs.umn.edu/resources/usr-248-dataset"><strong>USR-248:</strong> 
												        Dataset for Underwater Image Super-Resolution</a></li>
												<li><a href="http://irvlab.cs.umn.edu/resources/euvp-dataset"><strong>EUVP:</strong> 
												        Paired and Unpaired Samples for Underwater Image Enhancement</a></li>
											</ul>
										</li>-->
										<li>
											<span class="opener">Collaborations</span>
											<ul>
												<li><a href="#UGAN"><strong>UGAN:</strong> Underwater Image Enhancement by Generative Adversarial Networks</a></li>
												<li><a href="#CONVOY"><strong>Underwater Multi-Robot Convoying</strong> using Visual Tracking by Detection</a></li>
											</ul>
										</li>
										<li><a href="#Pub">List of Publications</a></li>
										<li><a href="#Indus">Industry Experience</a></li>
										<li><a href="#Awa">Awards and Achievements</a></li>
										<li><a href="files/resume_mji.pdf">Resume (Summer 2020)</a></li>
										<li><a href="#Extra">Weekend Tweakends!</a></li>
										
									</ul>
								</nav>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Recent Stories!</h2>
									</header>
									<div class="mini-posts">
										<article>
											<a href="https://roboticsconference.org/program/papers/18/" class="image"><img src="images/SESR_RSS_20.jpg" alt="" /></a>
											<p> <strong>July 2020:</strong> We presented <a href= "http://www.roboticsproceedings.org/rss16/p018.pdf">Deep SESR paper</a> 
											at the <a href= "https://roboticsconference.org/">RSS 2020 (virtual)</a>. 
											The workshops and technical sessions were really good.</p>
										</article>
										<article>
											<a href="https://github.com/xahidbuffon/SUIM-Net" class="image"><img src="images/suim_fig1.jpg" alt="" /></a>
											<p> <strong>June 2020:</strong> Our <a href= "https://arxiv.org/abs/2004.01241">SUIM-Net paper</a> 
											got accepted at the <a href= "https://www.iros2020.org/">IROS 2020</a>. We look forward to present it either 
											in Las Vegas or virtually (more likely).</p>
										</article>
										<article> 
											<a href="https://github.com/xahidbuffon/FUnIE-GAN" class="image"><img src="images/funie_1a.jpg" alt="" /></a>
											<p> <strong>Feb 2020:</strong> Our <a href= "https://ieeexplore.ieee.org/document/9001231">FUnIE-GAN paper</a> 
											is published at the <a href= "http://www.ieee-ras.org/publications/ra-l">RA-L</a>.   
											It is my fourth journal paper, and perhaps the most favourite one. [I.F.: 3.61]</p>
										</article>
										<article> 
											<a href="https://github.com/xahidbuffon/SRDRM" class="image"><img src="images/srdrm_1b.jpg" alt="" /></a>
											<p> <strong>Jan 2020:</strong> Our <a href= "https://arxiv.org/abs/1909.09437">SRDRM paper</a> got accepted 
											at the <a href= "http://icra2020.org/">ICRA 2020</a>. We are going to present this <del>in Paris</del> 
											virtually (guess why!). 
											</p>
										</article>
										<article> 
											<a href="https://arxiv.org/abs/1803.08202" class="image"><img src="images/_ppic_.jpg" alt="" /></a>
											<p> <strong>Oct 2019:</strong> Our extensive <a href= "https://journals.sagepub.com/doi/10.1177/0278364919881683">review paper</a> 
											on <i>person following by autonomous robots</i> is published at the <a href= "http://journals.sagepub.com/home/ijr">IJRR</a>. [I.F.: 6.134]</p>
										</article>
										<article> 
											<a href="https://www.qualcomm.com/" class="image"><img src="images/Qme.jpeg" alt="" /></a>
											<p> <strong>Sept 2019:</strong> Back to school after my internship at <a href= "https://www.qualcomm.com/">Qualcomm</a>; 
											had a great experience at work and a <i>summer well spent</i> in the Bay Area. </p>
										</article>
										<article> 
											<a href="https://www.cs.umn.edu/news/cse-phd-students-named-doctoral-dissertation-fellows-1" class="image"><img src="images/umn_cover.jpg" alt="" /></a>
											<p> <strong>Apr 2019:</strong> Received the prestigious <a href= "https://www.cs.umn.edu/news/cse-phd-students-named-doctoral-dissertation-fellows-1">
											Doctoral Dissertation Fellowship (DDF)</a> award for 2019-20 academic year.</p>
										<article>
										<p align="right">
											<a href="index.html"><span style="color: red;">
												<i class="fas fa-home"></i> <strong>Back to Top</strong></span>
											</a>
										</p>
									</div>
								</section>

							<!-- Section -->
								<!--<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="mailto: islam034@umn.edu">islam034@umn.edu</a></li>
										<li class="icon solid fa-briefcase"><a href="http://irvlab.cs.umn.edu/">Interactive Robotics and Vision Lab</a>, 
										150 Shepherd Laboratories, 100 Union St SE, Minneapolis, MN 55455, USA. </li>
									</ul>
								</section>-->

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Md Jahidul Islam. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
